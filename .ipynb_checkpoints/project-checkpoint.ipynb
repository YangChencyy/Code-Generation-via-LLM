{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789ddf16-1e92-42e6-b28f-ee55b41cfcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('code_search_net', 'python')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3570595-152a-427e-9785-f29106c42b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "        num_rows: 412178\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "        num_rows: 22176\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "        num_rows: 23107\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5674431e-f2c5-4d5e-b73d-1e52bfe1112c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'])\n",
      "Multiprocessing target for the zmq queue device\n",
      "ZeroMQReqServerChannel.zmq_device\n"
     ]
    }
   ],
   "source": [
    "sample = dataset['train'][0]\n",
    "print(sample.keys())\n",
    "print(sample['func_documentation_string'])\n",
    "print(sample['func_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24a84eb-fadf-4d3c-ba16-ecef4b3fb9bf",
   "metadata": {},
   "source": [
    "# Base T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0587d7f8-f01b-4e10-9e59-9d5cf2dbe224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: Give me python code to accomplish this function: Multiprocessing target for the zmq queue device\n",
      "Generated Text: to accomplish this function: to accomplish this function: to accomplish this function:\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the tokenizer and model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\")\n",
    "\n",
    "# Input text\n",
    "input_text = \"Give me python code to accomplish this function: \" + sample['func_documentation_string']\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode the output\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the result\n",
    "print(\"Input Text:\", input_text)\n",
    "print(\"Generated Text:\", decoded_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59789324-1e01-4874-9a4f-4ec52839268c",
   "metadata": {},
   "source": [
    "# Code T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca6a4268-ce39-41ae-ba1c-d48b9de53302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: Give me python code to accomplish this function: Multiprocessing target for the zmq queue device\n",
      "Generated Text:  public class MultiprocessingTarget {\n",
      "Correct Answer: def zmq_device(self):\n",
      "        '''\n",
      "        Multiprocessing target for the zmq queue device\n",
      "        '''\n",
      "        self.__setup_signals()\n",
      "        salt.utils.process.appendproctitle('MWorkerQueue')\n",
      "        self.context = zmq.Context(self.opts['worker_threads'])\n",
      "        # Prepare the zeromq sockets\n",
      "        self.uri = 'tcp://{interface}:{ret_port}'.format(**self.opts)\n",
      "        self.clients = self.context.socket(zmq.ROUTER)\n",
      "        if self.opts['ipv6'] is True and hasattr(zmq, 'IPV4ONLY'):\n",
      "            # IPv6 sockets work for both IPv6 and IPv4 addresses\n",
      "            self.clients.setsockopt(zmq.IPV4ONLY, 0)\n",
      "        self.clients.setsockopt(zmq.BACKLOG, self.opts.get('zmq_backlog', 1000))\n",
      "        self._start_zmq_monitor()\n",
      "        self.workers = self.context.socket(zmq.DEALER)\n",
      "\n",
      "        if self.opts.get('ipc_mode', '') == 'tcp':\n",
      "            self.w_uri = 'tcp://127.0.0.1:{0}'.format(\n",
      "                self.opts.get('tcp_master_workers', 4515)\n",
      "                )\n",
      "        else:\n",
      "            self.w_uri = 'ipc://{0}'.format(\n",
      "                os.path.join(self.opts['sock_dir'], 'workers.ipc')\n",
      "                )\n",
      "\n",
      "        log.info('Setting up the master communication server')\n",
      "        self.clients.bind(self.uri)\n",
      "        self.workers.bind(self.w_uri)\n",
      "\n",
      "        while True:\n",
      "            if self.clients.closed or self.workers.closed:\n",
      "                break\n",
      "            try:\n",
      "                zmq.device(zmq.QUEUE, self.clients, self.workers)\n",
      "            except zmq.ZMQError as exc:\n",
      "                if exc.errno == errno.EINTR:\n",
      "                    continue\n",
      "                raise exc\n",
      "            except (KeyboardInterrupt, SystemExit):\n",
      "                break\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the tokenizer and model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Salesforce/codet5-base\")\n",
    "\n",
    "# Input text\n",
    "input_text = \"Give me python code to accomplish this function: \" + sample['func_documentation_string']\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(**inputs, max_length=512)\n",
    "\n",
    "# Decode the output\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the result\n",
    "print(\"Input Text:\", input_text)\n",
    "print(\"Generated Text:\", decoded_output)\n",
    "print(\"Correct Answer:\", sample['func_code_string'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e175f2ff-d313-4c26-ab2c-23a7e0fcac17",
   "metadata": {},
   "source": [
    "# Qwen2.5 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c0cbfa-ae83-473a-a38d-8b147af5de40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Give me python code to accomplish this function: \" + sample['func_documentation_string']\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(\"Input Text:\", prompt)\n",
    "print(\"Generated Text:\", response)\n",
    "print(\"Correct Answer:\", sample['func_code_string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1688d0d-93e3-4661-b9ae-b1a3fd2f76c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project kernel",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
