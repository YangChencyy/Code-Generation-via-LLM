{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a10f1319-f2cd-494c-891d-5aecf56d6910",
   "metadata": {},
   "source": [
    "# Finetune with Humna Eval Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98ba3c9b-efe4-4bd0-bbc4-8a69d7bbbbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('code_search_net', 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e679288-f476-4896-a4cd-950f27f64dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_size = 10000\n",
    "train_dataset = dataset['train'].shuffle(seed=40).select(range(train_sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84978ce0-1f0e-4d6b-b43a-0ace5d7102b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item 6: Length = 813\n",
      "Item 7: Length = 933\n",
      "Item 8: Length = 1002\n",
      "Item 20: Length = 819\n",
      "Item 30: Length = 568\n",
      "Item 36: Length = 902\n",
      "Item 40: Length = 516\n",
      "Item 43: Length = 585\n",
      "Item 44: Length = 1999\n",
      "Item 52: Length = 505\n",
      "Item 56: Length = 584\n",
      "Item 58: Length = 556\n",
      "Item 62: Length = 2485\n",
      "Item 64: Length = 556\n",
      "Item 68: Length = 727\n",
      "Item 82: Length = 1120\n",
      "Item 83: Length = 805\n",
      "Item 91: Length = 1284\n",
      "Item 93: Length = 838\n",
      "Item 97: Length = 643\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    doc_string = train_dataset[i][\"func_documentation_string\"]\n",
    "    if len(doc_string) > 500:\n",
    "        print(f\"Item {i+1}: Length = {len(doc_string)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ebe1574-1475-4bfd-a4a0-b890f584dea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'repository_name': 'potatolondon/gae-pytz',\n",
       " 'func_path_in_repository': 'pytz/tzinfo.py',\n",
       " 'func_name': 'DstTzInfo.localize',\n",
       " 'whole_func_string': \"def localize(self, dt, is_dst=False):\\n        '''Convert naive time to local time.\\n\\n        This method should be used to construct localtimes, rather\\n        than passing a tzinfo argument to a datetime constructor.\\n\\n        is_dst is used to determine the correct timezone in the ambigous\\n        period at the end of daylight savings time.\\n\\n        >>> from pytz import timezone\\n        >>> fmt = '%Y-%m-%d %H:%M:%S %Z (%z)'\\n        >>> amdam = timezone('Europe/Amsterdam')\\n        >>> dt  = datetime(2004, 10, 31, 2, 0, 0)\\n        >>> loc_dt1 = amdam.localize(dt, is_dst=True)\\n        >>> loc_dt2 = amdam.localize(dt, is_dst=False)\\n        >>> loc_dt1.strftime(fmt)\\n        '2004-10-31 02:00:00 CEST (+0200)'\\n        >>> loc_dt2.strftime(fmt)\\n        '2004-10-31 02:00:00 CET (+0100)'\\n        >>> str(loc_dt2 - loc_dt1)\\n        '1:00:00'\\n\\n        Use is_dst=None to raise an AmbiguousTimeError for ambiguous\\n        times at the end of daylight savings\\n\\n        >>> loc_dt1 = amdam.localize(dt, is_dst=None)\\n        Traceback (most recent call last):\\n            [...]\\n        AmbiguousTimeError: 2004-10-31 02:00:00\\n\\n        is_dst defaults to False\\n\\n        >>> amdam.localize(dt) == amdam.localize(dt, False)\\n        True\\n\\n        is_dst is also used to determine the correct timezone in the\\n        wallclock times jumped over at the start of daylight savings time.\\n\\n        >>> pacific = timezone('US/Pacific')\\n        >>> dt = datetime(2008, 3, 9, 2, 0, 0)\\n        >>> ploc_dt1 = pacific.localize(dt, is_dst=True)\\n        >>> ploc_dt2 = pacific.localize(dt, is_dst=False)\\n        >>> ploc_dt1.strftime(fmt)\\n        '2008-03-09 02:00:00 PDT (-0700)'\\n        >>> ploc_dt2.strftime(fmt)\\n        '2008-03-09 02:00:00 PST (-0800)'\\n        >>> str(ploc_dt2 - ploc_dt1)\\n        '1:00:00'\\n\\n        Use is_dst=None to raise a NonExistentTimeError for these skipped\\n        times.\\n\\n        >>> loc_dt1 = pacific.localize(dt, is_dst=None)\\n        Traceback (most recent call last):\\n            [...]\\n        NonExistentTimeError: 2008-03-09 02:00:00\\n        '''\\n        if dt.tzinfo is not None:\\n            raise ValueError, 'Not naive datetime (tzinfo is already set)'\\n\\n        # Find the two best possibilities.\\n        possible_loc_dt = set()\\n        for delta in [timedelta(days=-1), timedelta(days=1)]:\\n            loc_dt = dt + delta\\n            idx = max(0, bisect_right(\\n                self._utc_transition_times, loc_dt) - 1)\\n            inf = self._transition_info[idx]\\n            tzinfo = self._tzinfos[inf]\\n            loc_dt = tzinfo.normalize(dt.replace(tzinfo=tzinfo))\\n            if loc_dt.replace(tzinfo=None) == dt:\\n                possible_loc_dt.add(loc_dt)\\n\\n        if len(possible_loc_dt) == 1:\\n            return possible_loc_dt.pop()\\n\\n        # If there are no possibly correct timezones, we are attempting\\n        # to convert a time that never happened - the time period jumped\\n        # during the start-of-DST transition period.\\n        if len(possible_loc_dt) == 0:\\n            # If we refuse to guess, raise an exception.\\n            if is_dst is None:\\n                raise NonExistentTimeError(dt)\\n\\n            # If we are forcing the pre-DST side of the DST transition, we\\n            # obtain the correct timezone by winding the clock forward a few\\n            # hours.\\n            elif is_dst:\\n                return self.localize(\\n                    dt + timedelta(hours=6), is_dst=True) - timedelta(hours=6)\\n\\n            # If we are forcing the post-DST side of the DST transition, we\\n            # obtain the correct timezone by winding the clock back.\\n            else:\\n                return self.localize(\\n                    dt - timedelta(hours=6), is_dst=False) + timedelta(hours=6)\\n\\n\\n        # If we get this far, we have multiple possible timezones - this\\n        # is an ambiguous case occuring during the end-of-DST transition.\\n\\n        # If told to be strict, raise an exception since we have an\\n        # ambiguous case\\n        if is_dst is None:\\n            raise AmbiguousTimeError(dt)\\n\\n        # Filter out the possiblilities that don't match the requested\\n        # is_dst\\n        filtered_possible_loc_dt = [\\n            p for p in possible_loc_dt\\n                if bool(p.tzinfo._dst) == is_dst\\n            ]\\n\\n        # Hopefully we only have one possibility left. Return it.\\n        if len(filtered_possible_loc_dt) == 1:\\n            return filtered_possible_loc_dt[0]\\n\\n        if len(filtered_possible_loc_dt) == 0:\\n            filtered_possible_loc_dt = list(possible_loc_dt)\\n\\n        # If we get this far, we have in a wierd timezone transition\\n        # where the clocks have been wound back but is_dst is the same\\n        # in both (eg. Europe/Warsaw 1915 when they switched to CET).\\n        # At this point, we just have to guess unless we allow more\\n        # hints to be passed in (such as the UTC offset or abbreviation),\\n        # but that is just getting silly.\\n        #\\n        # Choose the earliest (by UTC) applicable timezone.\\n        def mycmp(a,b):\\n            return cmp(\\n                    a.replace(tzinfo=None) - a.tzinfo._utcoffset,\\n                    b.replace(tzinfo=None) - b.tzinfo._utcoffset,\\n                    )\\n        filtered_possible_loc_dt.sort(mycmp)\\n        return filtered_possible_loc_dt[0]\",\n",
       " 'language': 'python',\n",
       " 'func_code_string': \"def localize(self, dt, is_dst=False):\\n        '''Convert naive time to local time.\\n\\n        This method should be used to construct localtimes, rather\\n        than passing a tzinfo argument to a datetime constructor.\\n\\n        is_dst is used to determine the correct timezone in the ambigous\\n        period at the end of daylight savings time.\\n\\n        >>> from pytz import timezone\\n        >>> fmt = '%Y-%m-%d %H:%M:%S %Z (%z)'\\n        >>> amdam = timezone('Europe/Amsterdam')\\n        >>> dt  = datetime(2004, 10, 31, 2, 0, 0)\\n        >>> loc_dt1 = amdam.localize(dt, is_dst=True)\\n        >>> loc_dt2 = amdam.localize(dt, is_dst=False)\\n        >>> loc_dt1.strftime(fmt)\\n        '2004-10-31 02:00:00 CEST (+0200)'\\n        >>> loc_dt2.strftime(fmt)\\n        '2004-10-31 02:00:00 CET (+0100)'\\n        >>> str(loc_dt2 - loc_dt1)\\n        '1:00:00'\\n\\n        Use is_dst=None to raise an AmbiguousTimeError for ambiguous\\n        times at the end of daylight savings\\n\\n        >>> loc_dt1 = amdam.localize(dt, is_dst=None)\\n        Traceback (most recent call last):\\n            [...]\\n        AmbiguousTimeError: 2004-10-31 02:00:00\\n\\n        is_dst defaults to False\\n\\n        >>> amdam.localize(dt) == amdam.localize(dt, False)\\n        True\\n\\n        is_dst is also used to determine the correct timezone in the\\n        wallclock times jumped over at the start of daylight savings time.\\n\\n        >>> pacific = timezone('US/Pacific')\\n        >>> dt = datetime(2008, 3, 9, 2, 0, 0)\\n        >>> ploc_dt1 = pacific.localize(dt, is_dst=True)\\n        >>> ploc_dt2 = pacific.localize(dt, is_dst=False)\\n        >>> ploc_dt1.strftime(fmt)\\n        '2008-03-09 02:00:00 PDT (-0700)'\\n        >>> ploc_dt2.strftime(fmt)\\n        '2008-03-09 02:00:00 PST (-0800)'\\n        >>> str(ploc_dt2 - ploc_dt1)\\n        '1:00:00'\\n\\n        Use is_dst=None to raise a NonExistentTimeError for these skipped\\n        times.\\n\\n        >>> loc_dt1 = pacific.localize(dt, is_dst=None)\\n        Traceback (most recent call last):\\n            [...]\\n        NonExistentTimeError: 2008-03-09 02:00:00\\n        '''\\n        if dt.tzinfo is not None:\\n            raise ValueError, 'Not naive datetime (tzinfo is already set)'\\n\\n        # Find the two best possibilities.\\n        possible_loc_dt = set()\\n        for delta in [timedelta(days=-1), timedelta(days=1)]:\\n            loc_dt = dt + delta\\n            idx = max(0, bisect_right(\\n                self._utc_transition_times, loc_dt) - 1)\\n            inf = self._transition_info[idx]\\n            tzinfo = self._tzinfos[inf]\\n            loc_dt = tzinfo.normalize(dt.replace(tzinfo=tzinfo))\\n            if loc_dt.replace(tzinfo=None) == dt:\\n                possible_loc_dt.add(loc_dt)\\n\\n        if len(possible_loc_dt) == 1:\\n            return possible_loc_dt.pop()\\n\\n        # If there are no possibly correct timezones, we are attempting\\n        # to convert a time that never happened - the time period jumped\\n        # during the start-of-DST transition period.\\n        if len(possible_loc_dt) == 0:\\n            # If we refuse to guess, raise an exception.\\n            if is_dst is None:\\n                raise NonExistentTimeError(dt)\\n\\n            # If we are forcing the pre-DST side of the DST transition, we\\n            # obtain the correct timezone by winding the clock forward a few\\n            # hours.\\n            elif is_dst:\\n                return self.localize(\\n                    dt + timedelta(hours=6), is_dst=True) - timedelta(hours=6)\\n\\n            # If we are forcing the post-DST side of the DST transition, we\\n            # obtain the correct timezone by winding the clock back.\\n            else:\\n                return self.localize(\\n                    dt - timedelta(hours=6), is_dst=False) + timedelta(hours=6)\\n\\n\\n        # If we get this far, we have multiple possible timezones - this\\n        # is an ambiguous case occuring during the end-of-DST transition.\\n\\n        # If told to be strict, raise an exception since we have an\\n        # ambiguous case\\n        if is_dst is None:\\n            raise AmbiguousTimeError(dt)\\n\\n        # Filter out the possiblilities that don't match the requested\\n        # is_dst\\n        filtered_possible_loc_dt = [\\n            p for p in possible_loc_dt\\n                if bool(p.tzinfo._dst) == is_dst\\n            ]\\n\\n        # Hopefully we only have one possibility left. Return it.\\n        if len(filtered_possible_loc_dt) == 1:\\n            return filtered_possible_loc_dt[0]\\n\\n        if len(filtered_possible_loc_dt) == 0:\\n            filtered_possible_loc_dt = list(possible_loc_dt)\\n\\n        # If we get this far, we have in a wierd timezone transition\\n        # where the clocks have been wound back but is_dst is the same\\n        # in both (eg. Europe/Warsaw 1915 when they switched to CET).\\n        # At this point, we just have to guess unless we allow more\\n        # hints to be passed in (such as the UTC offset or abbreviation),\\n        # but that is just getting silly.\\n        #\\n        # Choose the earliest (by UTC) applicable timezone.\\n        def mycmp(a,b):\\n            return cmp(\\n                    a.replace(tzinfo=None) - a.tzinfo._utcoffset,\\n                    b.replace(tzinfo=None) - b.tzinfo._utcoffset,\\n                    )\\n        filtered_possible_loc_dt.sort(mycmp)\\n        return filtered_possible_loc_dt[0]\",\n",
       " 'func_code_tokens': ['def',\n",
       "  'localize',\n",
       "  '(',\n",
       "  'self',\n",
       "  ',',\n",
       "  'dt',\n",
       "  ',',\n",
       "  'is_dst',\n",
       "  '=',\n",
       "  'False',\n",
       "  ')',\n",
       "  ':',\n",
       "  'if',\n",
       "  'dt',\n",
       "  '.',\n",
       "  'tzinfo',\n",
       "  'is',\n",
       "  'not',\n",
       "  'None',\n",
       "  ':',\n",
       "  'raise',\n",
       "  'ValueError',\n",
       "  ',',\n",
       "  \"'Not naive datetime (tzinfo is already set)'\",\n",
       "  '# Find the two best possibilities.',\n",
       "  'possible_loc_dt',\n",
       "  '=',\n",
       "  'set',\n",
       "  '(',\n",
       "  ')',\n",
       "  'for',\n",
       "  'delta',\n",
       "  'in',\n",
       "  '[',\n",
       "  'timedelta',\n",
       "  '(',\n",
       "  'days',\n",
       "  '=',\n",
       "  '-',\n",
       "  '1',\n",
       "  ')',\n",
       "  ',',\n",
       "  'timedelta',\n",
       "  '(',\n",
       "  'days',\n",
       "  '=',\n",
       "  '1',\n",
       "  ')',\n",
       "  ']',\n",
       "  ':',\n",
       "  'loc_dt',\n",
       "  '=',\n",
       "  'dt',\n",
       "  '+',\n",
       "  'delta',\n",
       "  'idx',\n",
       "  '=',\n",
       "  'max',\n",
       "  '(',\n",
       "  '0',\n",
       "  ',',\n",
       "  'bisect_right',\n",
       "  '(',\n",
       "  'self',\n",
       "  '.',\n",
       "  '_utc_transition_times',\n",
       "  ',',\n",
       "  'loc_dt',\n",
       "  ')',\n",
       "  '-',\n",
       "  '1',\n",
       "  ')',\n",
       "  'inf',\n",
       "  '=',\n",
       "  'self',\n",
       "  '.',\n",
       "  '_transition_info',\n",
       "  '[',\n",
       "  'idx',\n",
       "  ']',\n",
       "  'tzinfo',\n",
       "  '=',\n",
       "  'self',\n",
       "  '.',\n",
       "  '_tzinfos',\n",
       "  '[',\n",
       "  'inf',\n",
       "  ']',\n",
       "  'loc_dt',\n",
       "  '=',\n",
       "  'tzinfo',\n",
       "  '.',\n",
       "  'normalize',\n",
       "  '(',\n",
       "  'dt',\n",
       "  '.',\n",
       "  'replace',\n",
       "  '(',\n",
       "  'tzinfo',\n",
       "  '=',\n",
       "  'tzinfo',\n",
       "  ')',\n",
       "  ')',\n",
       "  'if',\n",
       "  'loc_dt',\n",
       "  '.',\n",
       "  'replace',\n",
       "  '(',\n",
       "  'tzinfo',\n",
       "  '=',\n",
       "  'None',\n",
       "  ')',\n",
       "  '==',\n",
       "  'dt',\n",
       "  ':',\n",
       "  'possible_loc_dt',\n",
       "  '.',\n",
       "  'add',\n",
       "  '(',\n",
       "  'loc_dt',\n",
       "  ')',\n",
       "  'if',\n",
       "  'len',\n",
       "  '(',\n",
       "  'possible_loc_dt',\n",
       "  ')',\n",
       "  '==',\n",
       "  '1',\n",
       "  ':',\n",
       "  'return',\n",
       "  'possible_loc_dt',\n",
       "  '.',\n",
       "  'pop',\n",
       "  '(',\n",
       "  ')',\n",
       "  '# If there are no possibly correct timezones, we are attempting',\n",
       "  '# to convert a time that never happened - the time period jumped',\n",
       "  '# during the start-of-DST transition period.',\n",
       "  'if',\n",
       "  'len',\n",
       "  '(',\n",
       "  'possible_loc_dt',\n",
       "  ')',\n",
       "  '==',\n",
       "  '0',\n",
       "  ':',\n",
       "  '# If we refuse to guess, raise an exception.',\n",
       "  'if',\n",
       "  'is_dst',\n",
       "  'is',\n",
       "  'None',\n",
       "  ':',\n",
       "  'raise',\n",
       "  'NonExistentTimeError',\n",
       "  '(',\n",
       "  'dt',\n",
       "  ')',\n",
       "  '# If we are forcing the pre-DST side of the DST transition, we',\n",
       "  '# obtain the correct timezone by winding the clock forward a few',\n",
       "  '# hours.',\n",
       "  'elif',\n",
       "  'is_dst',\n",
       "  ':',\n",
       "  'return',\n",
       "  'self',\n",
       "  '.',\n",
       "  'localize',\n",
       "  '(',\n",
       "  'dt',\n",
       "  '+',\n",
       "  'timedelta',\n",
       "  '(',\n",
       "  'hours',\n",
       "  '=',\n",
       "  '6',\n",
       "  ')',\n",
       "  ',',\n",
       "  'is_dst',\n",
       "  '=',\n",
       "  'True',\n",
       "  ')',\n",
       "  '-',\n",
       "  'timedelta',\n",
       "  '(',\n",
       "  'hours',\n",
       "  '=',\n",
       "  '6',\n",
       "  ')',\n",
       "  '# If we are forcing the post-DST side of the DST transition, we',\n",
       "  '# obtain the correct timezone by winding the clock back.',\n",
       "  'else',\n",
       "  ':',\n",
       "  'return',\n",
       "  'self',\n",
       "  '.',\n",
       "  'localize',\n",
       "  '(',\n",
       "  'dt',\n",
       "  '-',\n",
       "  'timedelta',\n",
       "  '(',\n",
       "  'hours',\n",
       "  '=',\n",
       "  '6',\n",
       "  ')',\n",
       "  ',',\n",
       "  'is_dst',\n",
       "  '=',\n",
       "  'False',\n",
       "  ')',\n",
       "  '+',\n",
       "  'timedelta',\n",
       "  '(',\n",
       "  'hours',\n",
       "  '=',\n",
       "  '6',\n",
       "  ')',\n",
       "  '# If we get this far, we have multiple possible timezones - this',\n",
       "  '# is an ambiguous case occuring during the end-of-DST transition.',\n",
       "  '# If told to be strict, raise an exception since we have an',\n",
       "  '# ambiguous case',\n",
       "  'if',\n",
       "  'is_dst',\n",
       "  'is',\n",
       "  'None',\n",
       "  ':',\n",
       "  'raise',\n",
       "  'AmbiguousTimeError',\n",
       "  '(',\n",
       "  'dt',\n",
       "  ')',\n",
       "  \"# Filter out the possiblilities that don't match the requested\",\n",
       "  '# is_dst',\n",
       "  'filtered_possible_loc_dt',\n",
       "  '=',\n",
       "  '[',\n",
       "  'p',\n",
       "  'for',\n",
       "  'p',\n",
       "  'in',\n",
       "  'possible_loc_dt',\n",
       "  'if',\n",
       "  'bool',\n",
       "  '(',\n",
       "  'p',\n",
       "  '.',\n",
       "  'tzinfo',\n",
       "  '.',\n",
       "  '_dst',\n",
       "  ')',\n",
       "  '==',\n",
       "  'is_dst',\n",
       "  ']',\n",
       "  '# Hopefully we only have one possibility left. Return it.',\n",
       "  'if',\n",
       "  'len',\n",
       "  '(',\n",
       "  'filtered_possible_loc_dt',\n",
       "  ')',\n",
       "  '==',\n",
       "  '1',\n",
       "  ':',\n",
       "  'return',\n",
       "  'filtered_possible_loc_dt',\n",
       "  '[',\n",
       "  '0',\n",
       "  ']',\n",
       "  'if',\n",
       "  'len',\n",
       "  '(',\n",
       "  'filtered_possible_loc_dt',\n",
       "  ')',\n",
       "  '==',\n",
       "  '0',\n",
       "  ':',\n",
       "  'filtered_possible_loc_dt',\n",
       "  '=',\n",
       "  'list',\n",
       "  '(',\n",
       "  'possible_loc_dt',\n",
       "  ')',\n",
       "  '# If we get this far, we have in a wierd timezone transition',\n",
       "  '# where the clocks have been wound back but is_dst is the same',\n",
       "  '# in both (eg. Europe/Warsaw 1915 when they switched to CET).',\n",
       "  '# At this point, we just have to guess unless we allow more',\n",
       "  '# hints to be passed in (such as the UTC offset or abbreviation),',\n",
       "  '# but that is just getting silly.',\n",
       "  '#',\n",
       "  '# Choose the earliest (by UTC) applicable timezone.',\n",
       "  'def',\n",
       "  'mycmp',\n",
       "  '(',\n",
       "  'a',\n",
       "  ',',\n",
       "  'b',\n",
       "  ')',\n",
       "  ':',\n",
       "  'return',\n",
       "  'cmp',\n",
       "  '(',\n",
       "  'a',\n",
       "  '.',\n",
       "  'replace',\n",
       "  '(',\n",
       "  'tzinfo',\n",
       "  '=',\n",
       "  'None',\n",
       "  ')',\n",
       "  '-',\n",
       "  'a',\n",
       "  '.',\n",
       "  'tzinfo',\n",
       "  '.',\n",
       "  '_utcoffset',\n",
       "  ',',\n",
       "  'b',\n",
       "  '.',\n",
       "  'replace',\n",
       "  '(',\n",
       "  'tzinfo',\n",
       "  '=',\n",
       "  'None',\n",
       "  ')',\n",
       "  '-',\n",
       "  'b',\n",
       "  '.',\n",
       "  'tzinfo',\n",
       "  '.',\n",
       "  '_utcoffset',\n",
       "  ',',\n",
       "  ')',\n",
       "  'filtered_possible_loc_dt',\n",
       "  '.',\n",
       "  'sort',\n",
       "  '(',\n",
       "  'mycmp',\n",
       "  ')',\n",
       "  'return',\n",
       "  'filtered_possible_loc_dt',\n",
       "  '[',\n",
       "  '0',\n",
       "  ']'],\n",
       " 'func_documentation_string': \"Convert naive time to local time.\\n\\n        This method should be used to construct localtimes, rather\\n        than passing a tzinfo argument to a datetime constructor.\\n\\n        is_dst is used to determine the correct timezone in the ambigous\\n        period at the end of daylight savings time.\\n\\n        >>> from pytz import timezone\\n        >>> fmt = '%Y-%m-%d %H:%M:%S %Z (%z)'\\n        >>> amdam = timezone('Europe/Amsterdam')\\n        >>> dt  = datetime(2004, 10, 31, 2, 0, 0)\\n        >>> loc_dt1 = amdam.localize(dt, is_dst=True)\\n        >>> loc_dt2 = amdam.localize(dt, is_dst=False)\\n        >>> loc_dt1.strftime(fmt)\\n        '2004-10-31 02:00:00 CEST (+0200)'\\n        >>> loc_dt2.strftime(fmt)\\n        '2004-10-31 02:00:00 CET (+0100)'\\n        >>> str(loc_dt2 - loc_dt1)\\n        '1:00:00'\\n\\n        Use is_dst=None to raise an AmbiguousTimeError for ambiguous\\n        times at the end of daylight savings\\n\\n        >>> loc_dt1 = amdam.localize(dt, is_dst=None)\\n        Traceback (most recent call last):\\n            [...]\\n        AmbiguousTimeError: 2004-10-31 02:00:00\\n\\n        is_dst defaults to False\\n\\n        >>> amdam.localize(dt) == amdam.localize(dt, False)\\n        True\\n\\n        is_dst is also used to determine the correct timezone in the\\n        wallclock times jumped over at the start of daylight savings time.\\n\\n        >>> pacific = timezone('US/Pacific')\\n        >>> dt = datetime(2008, 3, 9, 2, 0, 0)\\n        >>> ploc_dt1 = pacific.localize(dt, is_dst=True)\\n        >>> ploc_dt2 = pacific.localize(dt, is_dst=False)\\n        >>> ploc_dt1.strftime(fmt)\\n        '2008-03-09 02:00:00 PDT (-0700)'\\n        >>> ploc_dt2.strftime(fmt)\\n        '2008-03-09 02:00:00 PST (-0800)'\\n        >>> str(ploc_dt2 - ploc_dt1)\\n        '1:00:00'\\n\\n        Use is_dst=None to raise a NonExistentTimeError for these skipped\\n        times.\\n\\n        >>> loc_dt1 = pacific.localize(dt, is_dst=None)\\n        Traceback (most recent call last):\\n            [...]\\n        NonExistentTimeError: 2008-03-09 02:00:00\",\n",
       " 'func_documentation_tokens': ['Convert',\n",
       "  'naive',\n",
       "  'time',\n",
       "  'to',\n",
       "  'local',\n",
       "  'time',\n",
       "  '.'],\n",
       " 'split_name': 'train',\n",
       " 'func_code_url': 'https://github.com/potatolondon/gae-pytz/blob/24741951a7af3e79cd8727ae3f79265decc93fef/pytz/tzinfo.py#L193-L326'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eab01a9c-aeba-4a92-805a-d47ea8adbf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_sample_size = 5000\n",
    "val_dataset = dataset['validation'].shuffle(seed=40).select(range(val_sample_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7845fd0e-2c26-4617-987f-68edf1e53297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba4c228c92c4bc1a4304e88dc6e09a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from codebleu import calc_codebleu\n",
    "import torch\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17be3d54-12d1-4605-ad38-9398029a1067",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a438569-00b4-4dd2-a362-a2fc0d2959f1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight: Frozen\n",
      "model.layers.0.self_attn.q_proj.weight: Frozen\n",
      "model.layers.0.self_attn.q_proj.bias: Frozen\n",
      "model.layers.0.self_attn.k_proj.weight: Frozen\n",
      "model.layers.0.self_attn.k_proj.bias: Frozen\n",
      "model.layers.0.self_attn.v_proj.weight: Frozen\n",
      "model.layers.0.self_attn.v_proj.bias: Frozen\n",
      "model.layers.0.self_attn.o_proj.weight: Frozen\n",
      "model.layers.0.mlp.gate_proj.weight: Frozen\n",
      "model.layers.0.mlp.up_proj.weight: Frozen\n",
      "model.layers.0.mlp.down_proj.weight: Frozen\n",
      "model.layers.0.input_layernorm.weight: Frozen\n",
      "model.layers.0.post_attention_layernorm.weight: Frozen\n",
      "model.layers.1.self_attn.q_proj.weight: Frozen\n",
      "model.layers.1.self_attn.q_proj.bias: Frozen\n",
      "model.layers.1.self_attn.k_proj.weight: Frozen\n",
      "model.layers.1.self_attn.k_proj.bias: Frozen\n",
      "model.layers.1.self_attn.v_proj.weight: Frozen\n",
      "model.layers.1.self_attn.v_proj.bias: Frozen\n",
      "model.layers.1.self_attn.o_proj.weight: Frozen\n",
      "model.layers.1.mlp.gate_proj.weight: Frozen\n",
      "model.layers.1.mlp.up_proj.weight: Frozen\n",
      "model.layers.1.mlp.down_proj.weight: Frozen\n",
      "model.layers.1.input_layernorm.weight: Frozen\n",
      "model.layers.1.post_attention_layernorm.weight: Frozen\n",
      "model.layers.2.self_attn.q_proj.weight: Frozen\n",
      "model.layers.2.self_attn.q_proj.bias: Frozen\n",
      "model.layers.2.self_attn.k_proj.weight: Frozen\n",
      "model.layers.2.self_attn.k_proj.bias: Frozen\n",
      "model.layers.2.self_attn.v_proj.weight: Frozen\n",
      "model.layers.2.self_attn.v_proj.bias: Frozen\n",
      "model.layers.2.self_attn.o_proj.weight: Frozen\n",
      "model.layers.2.mlp.gate_proj.weight: Frozen\n",
      "model.layers.2.mlp.up_proj.weight: Frozen\n",
      "model.layers.2.mlp.down_proj.weight: Frozen\n",
      "model.layers.2.input_layernorm.weight: Frozen\n",
      "model.layers.2.post_attention_layernorm.weight: Frozen\n",
      "model.layers.3.self_attn.q_proj.weight: Frozen\n",
      "model.layers.3.self_attn.q_proj.bias: Frozen\n",
      "model.layers.3.self_attn.k_proj.weight: Frozen\n",
      "model.layers.3.self_attn.k_proj.bias: Frozen\n",
      "model.layers.3.self_attn.v_proj.weight: Frozen\n",
      "model.layers.3.self_attn.v_proj.bias: Frozen\n",
      "model.layers.3.self_attn.o_proj.weight: Frozen\n",
      "model.layers.3.mlp.gate_proj.weight: Frozen\n",
      "model.layers.3.mlp.up_proj.weight: Frozen\n",
      "model.layers.3.mlp.down_proj.weight: Frozen\n",
      "model.layers.3.input_layernorm.weight: Frozen\n",
      "model.layers.3.post_attention_layernorm.weight: Frozen\n",
      "model.layers.4.self_attn.q_proj.weight: Frozen\n",
      "model.layers.4.self_attn.q_proj.bias: Frozen\n",
      "model.layers.4.self_attn.k_proj.weight: Frozen\n",
      "model.layers.4.self_attn.k_proj.bias: Frozen\n",
      "model.layers.4.self_attn.v_proj.weight: Frozen\n",
      "model.layers.4.self_attn.v_proj.bias: Frozen\n",
      "model.layers.4.self_attn.o_proj.weight: Frozen\n",
      "model.layers.4.mlp.gate_proj.weight: Frozen\n",
      "model.layers.4.mlp.up_proj.weight: Frozen\n",
      "model.layers.4.mlp.down_proj.weight: Frozen\n",
      "model.layers.4.input_layernorm.weight: Frozen\n",
      "model.layers.4.post_attention_layernorm.weight: Frozen\n",
      "model.layers.5.self_attn.q_proj.weight: Frozen\n",
      "model.layers.5.self_attn.q_proj.bias: Frozen\n",
      "model.layers.5.self_attn.k_proj.weight: Frozen\n",
      "model.layers.5.self_attn.k_proj.bias: Frozen\n",
      "model.layers.5.self_attn.v_proj.weight: Frozen\n",
      "model.layers.5.self_attn.v_proj.bias: Frozen\n",
      "model.layers.5.self_attn.o_proj.weight: Frozen\n",
      "model.layers.5.mlp.gate_proj.weight: Frozen\n",
      "model.layers.5.mlp.up_proj.weight: Frozen\n",
      "model.layers.5.mlp.down_proj.weight: Frozen\n",
      "model.layers.5.input_layernorm.weight: Frozen\n",
      "model.layers.5.post_attention_layernorm.weight: Frozen\n",
      "model.layers.6.self_attn.q_proj.weight: Frozen\n",
      "model.layers.6.self_attn.q_proj.bias: Frozen\n",
      "model.layers.6.self_attn.k_proj.weight: Frozen\n",
      "model.layers.6.self_attn.k_proj.bias: Frozen\n",
      "model.layers.6.self_attn.v_proj.weight: Frozen\n",
      "model.layers.6.self_attn.v_proj.bias: Frozen\n",
      "model.layers.6.self_attn.o_proj.weight: Frozen\n",
      "model.layers.6.mlp.gate_proj.weight: Frozen\n",
      "model.layers.6.mlp.up_proj.weight: Frozen\n",
      "model.layers.6.mlp.down_proj.weight: Frozen\n",
      "model.layers.6.input_layernorm.weight: Frozen\n",
      "model.layers.6.post_attention_layernorm.weight: Frozen\n",
      "model.layers.7.self_attn.q_proj.weight: Frozen\n",
      "model.layers.7.self_attn.q_proj.bias: Frozen\n",
      "model.layers.7.self_attn.k_proj.weight: Frozen\n",
      "model.layers.7.self_attn.k_proj.bias: Frozen\n",
      "model.layers.7.self_attn.v_proj.weight: Frozen\n",
      "model.layers.7.self_attn.v_proj.bias: Frozen\n",
      "model.layers.7.self_attn.o_proj.weight: Frozen\n",
      "model.layers.7.mlp.gate_proj.weight: Frozen\n",
      "model.layers.7.mlp.up_proj.weight: Frozen\n",
      "model.layers.7.mlp.down_proj.weight: Frozen\n",
      "model.layers.7.input_layernorm.weight: Frozen\n",
      "model.layers.7.post_attention_layernorm.weight: Frozen\n",
      "model.layers.8.self_attn.q_proj.weight: Frozen\n",
      "model.layers.8.self_attn.q_proj.bias: Frozen\n",
      "model.layers.8.self_attn.k_proj.weight: Frozen\n",
      "model.layers.8.self_attn.k_proj.bias: Frozen\n",
      "model.layers.8.self_attn.v_proj.weight: Frozen\n",
      "model.layers.8.self_attn.v_proj.bias: Frozen\n",
      "model.layers.8.self_attn.o_proj.weight: Frozen\n",
      "model.layers.8.mlp.gate_proj.weight: Frozen\n",
      "model.layers.8.mlp.up_proj.weight: Frozen\n",
      "model.layers.8.mlp.down_proj.weight: Frozen\n",
      "model.layers.8.input_layernorm.weight: Frozen\n",
      "model.layers.8.post_attention_layernorm.weight: Frozen\n",
      "model.layers.9.self_attn.q_proj.weight: Frozen\n",
      "model.layers.9.self_attn.q_proj.bias: Frozen\n",
      "model.layers.9.self_attn.k_proj.weight: Frozen\n",
      "model.layers.9.self_attn.k_proj.bias: Frozen\n",
      "model.layers.9.self_attn.v_proj.weight: Frozen\n",
      "model.layers.9.self_attn.v_proj.bias: Frozen\n",
      "model.layers.9.self_attn.o_proj.weight: Frozen\n",
      "model.layers.9.mlp.gate_proj.weight: Frozen\n",
      "model.layers.9.mlp.up_proj.weight: Frozen\n",
      "model.layers.9.mlp.down_proj.weight: Frozen\n",
      "model.layers.9.input_layernorm.weight: Frozen\n",
      "model.layers.9.post_attention_layernorm.weight: Frozen\n",
      "model.layers.10.self_attn.q_proj.weight: Frozen\n",
      "model.layers.10.self_attn.q_proj.bias: Frozen\n",
      "model.layers.10.self_attn.k_proj.weight: Frozen\n",
      "model.layers.10.self_attn.k_proj.bias: Frozen\n",
      "model.layers.10.self_attn.v_proj.weight: Frozen\n",
      "model.layers.10.self_attn.v_proj.bias: Frozen\n",
      "model.layers.10.self_attn.o_proj.weight: Frozen\n",
      "model.layers.10.mlp.gate_proj.weight: Frozen\n",
      "model.layers.10.mlp.up_proj.weight: Frozen\n",
      "model.layers.10.mlp.down_proj.weight: Frozen\n",
      "model.layers.10.input_layernorm.weight: Frozen\n",
      "model.layers.10.post_attention_layernorm.weight: Frozen\n",
      "model.layers.11.self_attn.q_proj.weight: Frozen\n",
      "model.layers.11.self_attn.q_proj.bias: Frozen\n",
      "model.layers.11.self_attn.k_proj.weight: Frozen\n",
      "model.layers.11.self_attn.k_proj.bias: Frozen\n",
      "model.layers.11.self_attn.v_proj.weight: Frozen\n",
      "model.layers.11.self_attn.v_proj.bias: Frozen\n",
      "model.layers.11.self_attn.o_proj.weight: Frozen\n",
      "model.layers.11.mlp.gate_proj.weight: Frozen\n",
      "model.layers.11.mlp.up_proj.weight: Frozen\n",
      "model.layers.11.mlp.down_proj.weight: Frozen\n",
      "model.layers.11.input_layernorm.weight: Frozen\n",
      "model.layers.11.post_attention_layernorm.weight: Frozen\n",
      "model.layers.12.self_attn.q_proj.weight: Frozen\n",
      "model.layers.12.self_attn.q_proj.bias: Frozen\n",
      "model.layers.12.self_attn.k_proj.weight: Frozen\n",
      "model.layers.12.self_attn.k_proj.bias: Frozen\n",
      "model.layers.12.self_attn.v_proj.weight: Frozen\n",
      "model.layers.12.self_attn.v_proj.bias: Frozen\n",
      "model.layers.12.self_attn.o_proj.weight: Frozen\n",
      "model.layers.12.mlp.gate_proj.weight: Frozen\n",
      "model.layers.12.mlp.up_proj.weight: Frozen\n",
      "model.layers.12.mlp.down_proj.weight: Frozen\n",
      "model.layers.12.input_layernorm.weight: Frozen\n",
      "model.layers.12.post_attention_layernorm.weight: Frozen\n",
      "model.layers.13.self_attn.q_proj.weight: Frozen\n",
      "model.layers.13.self_attn.q_proj.bias: Frozen\n",
      "model.layers.13.self_attn.k_proj.weight: Frozen\n",
      "model.layers.13.self_attn.k_proj.bias: Frozen\n",
      "model.layers.13.self_attn.v_proj.weight: Frozen\n",
      "model.layers.13.self_attn.v_proj.bias: Frozen\n",
      "model.layers.13.self_attn.o_proj.weight: Frozen\n",
      "model.layers.13.mlp.gate_proj.weight: Frozen\n",
      "model.layers.13.mlp.up_proj.weight: Frozen\n",
      "model.layers.13.mlp.down_proj.weight: Frozen\n",
      "model.layers.13.input_layernorm.weight: Frozen\n",
      "model.layers.13.post_attention_layernorm.weight: Frozen\n",
      "model.layers.14.self_attn.q_proj.weight: Frozen\n",
      "model.layers.14.self_attn.q_proj.bias: Frozen\n",
      "model.layers.14.self_attn.k_proj.weight: Frozen\n",
      "model.layers.14.self_attn.k_proj.bias: Frozen\n",
      "model.layers.14.self_attn.v_proj.weight: Frozen\n",
      "model.layers.14.self_attn.v_proj.bias: Frozen\n",
      "model.layers.14.self_attn.o_proj.weight: Frozen\n",
      "model.layers.14.mlp.gate_proj.weight: Frozen\n",
      "model.layers.14.mlp.up_proj.weight: Frozen\n",
      "model.layers.14.mlp.down_proj.weight: Frozen\n",
      "model.layers.14.input_layernorm.weight: Frozen\n",
      "model.layers.14.post_attention_layernorm.weight: Frozen\n",
      "model.layers.15.self_attn.q_proj.weight: Frozen\n",
      "model.layers.15.self_attn.q_proj.bias: Frozen\n",
      "model.layers.15.self_attn.k_proj.weight: Frozen\n",
      "model.layers.15.self_attn.k_proj.bias: Frozen\n",
      "model.layers.15.self_attn.v_proj.weight: Frozen\n",
      "model.layers.15.self_attn.v_proj.bias: Frozen\n",
      "model.layers.15.self_attn.o_proj.weight: Frozen\n",
      "model.layers.15.mlp.gate_proj.weight: Frozen\n",
      "model.layers.15.mlp.up_proj.weight: Frozen\n",
      "model.layers.15.mlp.down_proj.weight: Frozen\n",
      "model.layers.15.input_layernorm.weight: Frozen\n",
      "model.layers.15.post_attention_layernorm.weight: Frozen\n",
      "model.layers.16.self_attn.q_proj.weight: Frozen\n",
      "model.layers.16.self_attn.q_proj.bias: Frozen\n",
      "model.layers.16.self_attn.k_proj.weight: Frozen\n",
      "model.layers.16.self_attn.k_proj.bias: Frozen\n",
      "model.layers.16.self_attn.v_proj.weight: Frozen\n",
      "model.layers.16.self_attn.v_proj.bias: Frozen\n",
      "model.layers.16.self_attn.o_proj.weight: Frozen\n",
      "model.layers.16.mlp.gate_proj.weight: Frozen\n",
      "model.layers.16.mlp.up_proj.weight: Frozen\n",
      "model.layers.16.mlp.down_proj.weight: Frozen\n",
      "model.layers.16.input_layernorm.weight: Frozen\n",
      "model.layers.16.post_attention_layernorm.weight: Frozen\n",
      "model.layers.17.self_attn.q_proj.weight: Frozen\n",
      "model.layers.17.self_attn.q_proj.bias: Frozen\n",
      "model.layers.17.self_attn.k_proj.weight: Frozen\n",
      "model.layers.17.self_attn.k_proj.bias: Frozen\n",
      "model.layers.17.self_attn.v_proj.weight: Frozen\n",
      "model.layers.17.self_attn.v_proj.bias: Frozen\n",
      "model.layers.17.self_attn.o_proj.weight: Frozen\n",
      "model.layers.17.mlp.gate_proj.weight: Frozen\n",
      "model.layers.17.mlp.up_proj.weight: Frozen\n",
      "model.layers.17.mlp.down_proj.weight: Frozen\n",
      "model.layers.17.input_layernorm.weight: Frozen\n",
      "model.layers.17.post_attention_layernorm.weight: Frozen\n",
      "model.layers.18.self_attn.q_proj.weight: Trainable\n",
      "model.layers.18.self_attn.q_proj.bias: Trainable\n",
      "model.layers.18.self_attn.k_proj.weight: Trainable\n",
      "model.layers.18.self_attn.k_proj.bias: Trainable\n",
      "model.layers.18.self_attn.v_proj.weight: Trainable\n",
      "model.layers.18.self_attn.v_proj.bias: Trainable\n",
      "model.layers.18.self_attn.o_proj.weight: Trainable\n",
      "model.layers.18.mlp.gate_proj.weight: Trainable\n",
      "model.layers.18.mlp.up_proj.weight: Trainable\n",
      "model.layers.18.mlp.down_proj.weight: Trainable\n",
      "model.layers.18.input_layernorm.weight: Trainable\n",
      "model.layers.18.post_attention_layernorm.weight: Trainable\n",
      "model.layers.19.self_attn.q_proj.weight: Trainable\n",
      "model.layers.19.self_attn.q_proj.bias: Trainable\n",
      "model.layers.19.self_attn.k_proj.weight: Trainable\n",
      "model.layers.19.self_attn.k_proj.bias: Trainable\n",
      "model.layers.19.self_attn.v_proj.weight: Trainable\n",
      "model.layers.19.self_attn.v_proj.bias: Trainable\n",
      "model.layers.19.self_attn.o_proj.weight: Trainable\n",
      "model.layers.19.mlp.gate_proj.weight: Trainable\n",
      "model.layers.19.mlp.up_proj.weight: Trainable\n",
      "model.layers.19.mlp.down_proj.weight: Trainable\n",
      "model.layers.19.input_layernorm.weight: Trainable\n",
      "model.layers.19.post_attention_layernorm.weight: Trainable\n",
      "model.layers.20.self_attn.q_proj.weight: Trainable\n",
      "model.layers.20.self_attn.q_proj.bias: Trainable\n",
      "model.layers.20.self_attn.k_proj.weight: Trainable\n",
      "model.layers.20.self_attn.k_proj.bias: Trainable\n",
      "model.layers.20.self_attn.v_proj.weight: Trainable\n",
      "model.layers.20.self_attn.v_proj.bias: Trainable\n",
      "model.layers.20.self_attn.o_proj.weight: Trainable\n",
      "model.layers.20.mlp.gate_proj.weight: Trainable\n",
      "model.layers.20.mlp.up_proj.weight: Trainable\n",
      "model.layers.20.mlp.down_proj.weight: Trainable\n",
      "model.layers.20.input_layernorm.weight: Trainable\n",
      "model.layers.20.post_attention_layernorm.weight: Trainable\n",
      "model.layers.21.self_attn.q_proj.weight: Trainable\n",
      "model.layers.21.self_attn.q_proj.bias: Trainable\n",
      "model.layers.21.self_attn.k_proj.weight: Trainable\n",
      "model.layers.21.self_attn.k_proj.bias: Trainable\n",
      "model.layers.21.self_attn.v_proj.weight: Trainable\n",
      "model.layers.21.self_attn.v_proj.bias: Trainable\n",
      "model.layers.21.self_attn.o_proj.weight: Trainable\n",
      "model.layers.21.mlp.gate_proj.weight: Trainable\n",
      "model.layers.21.mlp.up_proj.weight: Trainable\n",
      "model.layers.21.mlp.down_proj.weight: Trainable\n",
      "model.layers.21.input_layernorm.weight: Trainable\n",
      "model.layers.21.post_attention_layernorm.weight: Trainable\n",
      "model.layers.22.self_attn.q_proj.weight: Trainable\n",
      "model.layers.22.self_attn.q_proj.bias: Trainable\n",
      "model.layers.22.self_attn.k_proj.weight: Trainable\n",
      "model.layers.22.self_attn.k_proj.bias: Trainable\n",
      "model.layers.22.self_attn.v_proj.weight: Trainable\n",
      "model.layers.22.self_attn.v_proj.bias: Trainable\n",
      "model.layers.22.self_attn.o_proj.weight: Trainable\n",
      "model.layers.22.mlp.gate_proj.weight: Trainable\n",
      "model.layers.22.mlp.up_proj.weight: Trainable\n",
      "model.layers.22.mlp.down_proj.weight: Trainable\n",
      "model.layers.22.input_layernorm.weight: Trainable\n",
      "model.layers.22.post_attention_layernorm.weight: Trainable\n",
      "model.layers.23.self_attn.q_proj.weight: Trainable\n",
      "model.layers.23.self_attn.q_proj.bias: Trainable\n",
      "model.layers.23.self_attn.k_proj.weight: Trainable\n",
      "model.layers.23.self_attn.k_proj.bias: Trainable\n",
      "model.layers.23.self_attn.v_proj.weight: Trainable\n",
      "model.layers.23.self_attn.v_proj.bias: Trainable\n",
      "model.layers.23.self_attn.o_proj.weight: Trainable\n",
      "model.layers.23.mlp.gate_proj.weight: Trainable\n",
      "model.layers.23.mlp.up_proj.weight: Trainable\n",
      "model.layers.23.mlp.down_proj.weight: Trainable\n",
      "model.layers.23.input_layernorm.weight: Trainable\n",
      "model.layers.23.post_attention_layernorm.weight: Trainable\n",
      "model.layers.24.self_attn.q_proj.weight: Trainable\n",
      "model.layers.24.self_attn.q_proj.bias: Trainable\n",
      "model.layers.24.self_attn.k_proj.weight: Trainable\n",
      "model.layers.24.self_attn.k_proj.bias: Trainable\n",
      "model.layers.24.self_attn.v_proj.weight: Trainable\n",
      "model.layers.24.self_attn.v_proj.bias: Trainable\n",
      "model.layers.24.self_attn.o_proj.weight: Trainable\n",
      "model.layers.24.mlp.gate_proj.weight: Trainable\n",
      "model.layers.24.mlp.up_proj.weight: Trainable\n",
      "model.layers.24.mlp.down_proj.weight: Trainable\n",
      "model.layers.24.input_layernorm.weight: Trainable\n",
      "model.layers.24.post_attention_layernorm.weight: Trainable\n",
      "model.layers.25.self_attn.q_proj.weight: Trainable\n",
      "model.layers.25.self_attn.q_proj.bias: Trainable\n",
      "model.layers.25.self_attn.k_proj.weight: Trainable\n",
      "model.layers.25.self_attn.k_proj.bias: Trainable\n",
      "model.layers.25.self_attn.v_proj.weight: Trainable\n",
      "model.layers.25.self_attn.v_proj.bias: Trainable\n",
      "model.layers.25.self_attn.o_proj.weight: Trainable\n",
      "model.layers.25.mlp.gate_proj.weight: Trainable\n",
      "model.layers.25.mlp.up_proj.weight: Trainable\n",
      "model.layers.25.mlp.down_proj.weight: Trainable\n",
      "model.layers.25.input_layernorm.weight: Trainable\n",
      "model.layers.25.post_attention_layernorm.weight: Trainable\n",
      "model.layers.26.self_attn.q_proj.weight: Trainable\n",
      "model.layers.26.self_attn.q_proj.bias: Trainable\n",
      "model.layers.26.self_attn.k_proj.weight: Trainable\n",
      "model.layers.26.self_attn.k_proj.bias: Trainable\n",
      "model.layers.26.self_attn.v_proj.weight: Trainable\n",
      "model.layers.26.self_attn.v_proj.bias: Trainable\n",
      "model.layers.26.self_attn.o_proj.weight: Trainable\n",
      "model.layers.26.mlp.gate_proj.weight: Trainable\n",
      "model.layers.26.mlp.up_proj.weight: Trainable\n",
      "model.layers.26.mlp.down_proj.weight: Trainable\n",
      "model.layers.26.input_layernorm.weight: Trainable\n",
      "model.layers.26.post_attention_layernorm.weight: Trainable\n",
      "model.layers.27.self_attn.q_proj.weight: Trainable\n",
      "model.layers.27.self_attn.q_proj.bias: Trainable\n",
      "model.layers.27.self_attn.k_proj.weight: Trainable\n",
      "model.layers.27.self_attn.k_proj.bias: Trainable\n",
      "model.layers.27.self_attn.v_proj.weight: Trainable\n",
      "model.layers.27.self_attn.v_proj.bias: Trainable\n",
      "model.layers.27.self_attn.o_proj.weight: Trainable\n",
      "model.layers.27.mlp.gate_proj.weight: Trainable\n",
      "model.layers.27.mlp.up_proj.weight: Trainable\n",
      "model.layers.27.mlp.down_proj.weight: Trainable\n",
      "model.layers.27.input_layernorm.weight: Trainable\n",
      "model.layers.27.post_attention_layernorm.weight: Trainable\n",
      "model.norm.weight: Frozen\n",
      "lm_head.weight: Frozen\n"
     ]
    }
   ],
   "source": [
    "def freeze_layers(model, start_layer, end_layer):\n",
    "    for name, param in model.named_parameters():\n",
    "        layer_num = None\n",
    "        if \"layers.\" in name:\n",
    "            try:\n",
    "                layer_num = int(name.split(\"layers.\")[1].split(\".\")[0])\n",
    "            except (IndexError, ValueError):\n",
    "                pass\n",
    "        \n",
    "        if layer_num is not None and start_layer <= layer_num <= end_layer:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False\n",
    "\n",
    "freeze_layers(model, start_layer=18, end_layer=27)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {'Trainable' if param.requires_grad else 'Frozen'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2487791-8ef0-41a0-958c-58c4e94e2972",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c32bc9e4e540e7a577686309976bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"func_documentation_string\"]\n",
    "    outputs = examples[\"func_code_string\"]\n",
    "\n",
    "    formatted_inputs = f\"Generate code for the following documentation:\\n{inputs}\\n\\n### Code:\\n\"\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        formatted_inputs,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    # Tokenize outputs (labels)\n",
    "    labels = tokenizer(\n",
    "        outputs,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    # Add labels to model inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "train_dataset.reset_format()\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=False,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    load_from_cache_file=False \n",
    ")\n",
    "tokenized_train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# # LoRA Configuration\n",
    "# lora_config = LoraConfig(\n",
    "#     r=8,  # LoRA rank\n",
    "#     lora_alpha=32,  # Scaling factor\n",
    "#     target_modules=[\"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\"],  # Correct target modules\n",
    "#     lora_dropout=0.1,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\"\n",
    "# )\n",
    "\n",
    "# # Apply LoRA to the model\n",
    "# model = get_peft_model(model, lora_config)\n",
    "# model.print_trainable_parameters()  # Check trainable parameters count\n",
    "\n",
    "# # Define training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./lora_finetuned_model\",\n",
    "#     per_device_train_batch_size=1,\n",
    "#     gradient_accumulation_steps=4,\n",
    "#     num_train_epochs=1,\n",
    "#     learning_rate=2e-4,\n",
    "#     fp16=True,\n",
    "#     logging_dir=\"./logs\",\n",
    "#     logging_steps=10,\n",
    "#     save_strategy=\"epoch\",\n",
    "#     save_total_limit=1,\n",
    "#     report_to=\"none\"\n",
    "# )\n",
    "\n",
    "# # Trainer setup\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_train_dataset\n",
    "# )\n",
    "\n",
    "# # Fine-tune the model\n",
    "# trainer.train()\n",
    "\n",
    "# # Save the LoRA-adapted model\n",
    "# trainer.save_model(\"./lora_finetuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2b0892c-c8c1-4459-93f3-e27ab174ab2c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target module ModuleList(\n  (0-27): 28 x Qwen2DecoderLayer(\n    (self_attn): Qwen2SdpaAttention(\n      (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n      (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n      (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n      (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n      (rotary_emb): Qwen2RotaryEmbedding()\n    )\n    (mlp): Qwen2MLP(\n      (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n      (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n      (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n      (act_fn): SiLU()\n    )\n    (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n    (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n  )\n) is not supported. Currently, only the following modules are supported: `torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 34\u001b[0m\n\u001b[1;32m     23\u001b[0m filtered_target_modules \u001b[38;5;241m=\u001b[39m get_lora_target_modules(model, start_layer, end_layer)\n\u001b[1;32m     25\u001b[0m lora_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m     26\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m     27\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAUSAL_LM\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 34\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mget_peft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39mprint_trainable_parameters()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/peft/mapping.py:183\u001b[0m, in \u001b[0;36mget_peft_model\u001b[0;34m(model, peft_config, adapter_name, mixed, autocast_adapter_dtype, revision)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mis_prompt_learning:\n\u001b[1;32m    182\u001b[0m     peft_config \u001b[38;5;241m=\u001b[39m _prepare_prompt_learning_config(peft_config, model_config)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMODEL_TYPE_TO_PEFT_MODEL_MAPPING\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautocast_adapter_dtype\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/peft/peft_model.py:1542\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.__init__\u001b[0;34m(self, model, peft_config, adapter_name, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m   1540\u001b[0m     \u001b[38;5;28mself\u001b[39m, model: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, peft_config: PeftConfig, adapter_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m   1541\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1542\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/peft/peft_model.py:155\u001b[0m, in \u001b[0;36mPeftModel.__init__\u001b[0;34m(self, model, peft_config, adapter_name, autocast_adapter_dtype)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_peft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m PEFT_TYPE_TO_MODEL_MAPPING[peft_config\u001b[38;5;241m.\u001b[39mpeft_type]\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_additional_trainable_modules(peft_config, adapter_name)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cast_adapter_dtype\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/peft/tuners/lora/model.py:139\u001b[0m, in \u001b[0;36mLoraModel.__init__\u001b[0;34m(self, model, config, adapter_name)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, config, adapter_name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:175\u001b[0m, in \u001b[0;36mBaseTuner.__init__\u001b[0;34m(self, model, peft_config, adapter_name)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_injection_hook(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config[adapter_name], adapter_name)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config \u001b[38;5;241m!=\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mXLORA \u001b[38;5;129;01mor\u001b[39;00m peft_config[adapter_name] \u001b[38;5;241m!=\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mXLORA:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minject_adapter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Copy the peft_config in the injected model.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpeft_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_config\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:431\u001b[0m, in \u001b[0;36mBaseTuner.inject_adapter\u001b[0;34m(self, model, adapter_name, autocast_adapter_dtype)\u001b[0m\n\u001b[1;32m    429\u001b[0m     is_target_modules_in_base_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m     parent, target, target_name \u001b[38;5;241m=\u001b[39m _get_submodules(model, key)\n\u001b[0;32m--> 431\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_and_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# Handle X-LoRA case.\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_target_modules_in_base_model \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(peft_config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_modules\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/peft/tuners/lora/model.py:224\u001b[0m, in \u001b[0;36mLoraModel._create_and_replace\u001b[0;34m(self, lora_config, adapter_name, target, target_name, parent, current_key)\u001b[0m\n\u001b[1;32m    214\u001b[0m     target\u001b[38;5;241m.\u001b[39mupdate_layer(\n\u001b[1;32m    215\u001b[0m         adapter_name,\n\u001b[1;32m    216\u001b[0m         r,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    221\u001b[0m         use_dora\u001b[38;5;241m=\u001b[39mlora_config\u001b[38;5;241m.\u001b[39muse_dora,\n\u001b[1;32m    222\u001b[0m     )\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 224\u001b[0m     new_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m adapter_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactive_adapters:\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;66;03m# adding an additional adapter: it is not automatically trainable\u001b[39;00m\n\u001b[1;32m    227\u001b[0m         new_module\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/peft/tuners/lora/model.py:346\u001b[0m, in \u001b[0;36mLoraModel._create_new_module\u001b[0;34m(lora_config, adapter_name, target, **kwargs)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;66;03m# no module could be matched\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported. Currently, only the following modules are supported: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    348\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    349\u001b[0m     )\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_module\n",
      "\u001b[0;31mValueError\u001b[0m: Target module ModuleList(\n  (0-27): 28 x Qwen2DecoderLayer(\n    (self_attn): Qwen2SdpaAttention(\n      (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n      (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n      (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n      (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n      (rotary_emb): Qwen2RotaryEmbedding()\n    )\n    (mlp): Qwen2MLP(\n      (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n      (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n      (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n      (act_fn): SiLU()\n    )\n    (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n    (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n  )\n) is not supported. Currently, only the following modules are supported: `torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`."
     ]
    }
   ],
   "source": [
    "def get_lora_target_modules(model, start_layer, end_layer):\n",
    "    target_modules = []\n",
    "    for name, _ in model.named_parameters():\n",
    "        layer_num = None\n",
    "        if \"layers.\" in name:  # Adjust based on model architecture\n",
    "            try:\n",
    "                layer_num = int(name.split(\"layers.\")[1].split(\".\")[0])\n",
    "            except (IndexError, ValueError):\n",
    "                pass\n",
    "        \n",
    "        if layer_num is not None and start_layer <= layer_num <= end_layer:\n",
    "            if \"q_proj\" in name or \"k_proj\" in name or \"v_proj\" in name:\n",
    "                module_name = name.split(\".\")[0] + \".\" + name.split(\".\")[1]  # Extract the module name\n",
    "                if module_name not in target_modules:\n",
    "                    target_modules.append(module_name)\n",
    "    \n",
    "    return target_modules\n",
    "\n",
    "# Define the start and end layers for LoRA\n",
    "start_layer = 18\n",
    "end_layer = 27\n",
    "\n",
    "filtered_target_modules = get_lora_target_modules(model, start_layer, end_layer)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"self_attn.q_proj\",\n",
    "        \"self_attn.k_proj\",\n",
    "        \"self_attn.v_proj\"\n",
    "        ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad2dd19-254c-49bd-ba61-b36659b6199d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_finetuned_model\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the LoRA-adapted model\n",
    "trainer.save_model(\"./lora_finetuned_model_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a55b6f-a6d8-4b3c-a180-2ae37cc82acd",
   "metadata": {},
   "source": [
    "# No LORA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da2b2e3-ab4b-4921-a1e1-fb881fae6a43",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_inputs\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Tokenize and format the dataset\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241m.\u001b[39mreset_format()\n\u001b[1;32m     35\u001b[0m tokenized_train_dataset \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m     36\u001b[0m     preprocess_function,\n\u001b[1;32m     37\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     38\u001b[0m     remove_columns\u001b[38;5;241m=\u001b[39mtrain_dataset\u001b[38;5;241m.\u001b[39mcolumn_names,\n\u001b[1;32m     39\u001b[0m     load_from_cache_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     41\u001b[0m tokenized_train_dataset\u001b[38;5;241m.\u001b[39mset_format(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import PrefixTuningConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[\"func_documentation_string\"]\n",
    "    outputs = examples[\"func_code_string\"]\n",
    "\n",
    "    formatted_inputs = f\"Generate code for the following documentation:\\n{inputs}\\n\\n### Code:\\n\"\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        formatted_inputs,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024\n",
    "    )\n",
    "    \n",
    "    # Tokenize outputs (labels)\n",
    "    labels = tokenizer(\n",
    "        outputs,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=1024\n",
    "    )\n",
    "    \n",
    "    # Add labels to model inputs\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize and format the dataset\n",
    "train_dataset.reset_format()\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=False,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    load_from_cache_file=False \n",
    ")\n",
    "tokenized_train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "prefix_config = PrefixTuningConfig(\n",
    "    task_type=\"CAUSAL_LM\",  # Type of task\n",
    "    num_virtual_tokens=30,  # Number of virtual tokens to prepend\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, prefix_config)\n",
    "model.print_trainable_parameters()  # Check trainable parameters count\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./prefix_tuned_model\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the Prefix Tuned model\n",
    "trainer.save_model(\"./prefix_tuned_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849e4d9d-a1bb-46a4-99f1-cbb45cbc9a14",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c472abf0-0f35-4379-ab01-213c4f50ebfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e305f6d3aa4d978a518a620610896b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "/opt/conda/lib/python3.12/site-packages/peft/peft_model.py:1685: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n",
      "  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Code:\n",
      " Human: \n",
      "____\n",
      "A. \n",
      "B. \n",
      "C. \n",
      ":\n",
      "\n",
      "Assistant: C\n",
      "\n",
      "Human: \n",
      "____\n",
      "A. \n",
      "B. \n",
      "C. \n",
      ":\n",
      "\n",
      "Assistant: C\n",
      "\n",
      "Human: \n",
      "____\n",
      "A. \n",
      "B. \n",
      "C. \n",
      ":\n",
      "\n",
      "Assistant: C\n",
      "\n",
      "Human: \n",
      "____\n",
      "A. \n",
      "B. \n",
      "C. \n",
      ":\n",
      "\n",
      "Assistant: C\n",
      "\n",
      "Human: \n",
      "____\n",
      "A. \n",
      "B. \n",
      "C. \n",
      ":\n",
      "\n",
      "Assistant: C\n",
      "\n",
      "Human: \n",
      "____\n",
      "A. \n",
      "B. \n",
      "C. \n",
      ":\n",
      "\n",
      "Assistant: C\n",
      "\n",
      "Human: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "from codebleu import calc_codebleu\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model_name = \"Qwen/Qwen2.5-7B-Instruct\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "\n",
    "# Load the LoRA-adapted model\n",
    "model = PeftModel.from_pretrained(base_model, \"./prefix_tuned_model\")\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"Generate code for the following documentation:\\nCreate a function that adds two numbers.\\n\\n### Code:\\n\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(\n",
    "    test_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=512,\n",
    "    num_beams=5,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# Decode and print\n",
    "generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Code:\\n\", generated_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e7da9b-9000-4db8-a601-18a23d59a66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24309b9e3cd424a85c374fec6e53e99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Code:\n",
      " Generate code for the following documentation:\n",
      "Create a function that multiply two numbers.\n",
      "\n",
      "### Code:\n",
      " def two two multiply two multiply two multiply two multiply multiply multiply two multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply multiply\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "# Load the base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-7B-Instruct\")\n",
    "# Load LoRA weights into the base model\n",
    "model = PeftModel.from_pretrained(base_model, \"./lora_finetuned_model\")\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"Generate code for the following documentation:\\nCreate a function that multiply two numbers.\\n\\n### Code:\\n\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(\n",
    "    test_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=256\n",
    ").to('cuda')  # Ensure inputs are on the same device as the model\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=256,\n",
    "    do_sample=True,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "# Decode and print\n",
    "generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Code:\\n\", generated_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4376a2ba-b666-4369-aa7d-97cc9fb16977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompt\n",
    "test_prompt = \"Generate code for the following documentation:\\nCreate a function that download video from website.\\n\\n### Code:\\n\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(\n",
    "    test_prompt,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=256\n",
    ").to('cuda')  # Ensure inputs are on the same device as the model\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_length=256,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.5  # Penalize token repetition\n",
    ")\n",
    "\n",
    "# Decode and print\n",
    "generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"Generated Code:\\n\", generated_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e654eb84-1543-4227-aa29-370a7c4bc84a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project kernel",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
