{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789ddf16-1e92-42e6-b28f-ee55b41cfcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('code_search_net', 'python')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3570595-152a-427e-9785-f29106c42b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "        num_rows: 412178\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "        num_rows: 22176\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "        num_rows: 23107\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5674431e-f2c5-4d5e-b73d-1e52bfe1112c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'])\n",
      "Multiprocessing target for the zmq queue device\n",
      "ZeroMQReqServerChannel.zmq_device\n"
     ]
    }
   ],
   "source": [
    "sample = dataset['train'][0]\n",
    "print(sample.keys())\n",
    "print(sample['func_documentation_string'])\n",
    "print(sample['func_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24a84eb-fadf-4d3c-ba16-ecef4b3fb9bf",
   "metadata": {},
   "source": [
    "# Base T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0587d7f8-f01b-4e10-9e59-9d5cf2dbe224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: Give me python code to accomplish this function: Multiprocessing target for the zmq queue device\n",
      "Generated Text: to accomplish this function: to accomplish this function: to accomplish this function:\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the tokenizer and model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\")\n",
    "\n",
    "# Input text\n",
    "input_text = \"Give me python code to accomplish this function: \" + sample['func_documentation_string']\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(**inputs)\n",
    "\n",
    "# Decode the output\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the result\n",
    "print(\"Input Text:\", input_text)\n",
    "print(\"Generated Text:\", decoded_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59789324-1e01-4874-9a4f-4ec52839268c",
   "metadata": {},
   "source": [
    "# Code T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca6a4268-ce39-41ae-ba1c-d48b9de53302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: Give me python code to accomplish this function: Multiprocessing target for the zmq queue device\n",
      "Generated Text:  public class MultiprocessingTarget {\n",
      "Correct Answer: def zmq_device(self):\n",
      "        '''\n",
      "        Multiprocessing target for the zmq queue device\n",
      "        '''\n",
      "        self.__setup_signals()\n",
      "        salt.utils.process.appendproctitle('MWorkerQueue')\n",
      "        self.context = zmq.Context(self.opts['worker_threads'])\n",
      "        # Prepare the zeromq sockets\n",
      "        self.uri = 'tcp://{interface}:{ret_port}'.format(**self.opts)\n",
      "        self.clients = self.context.socket(zmq.ROUTER)\n",
      "        if self.opts['ipv6'] is True and hasattr(zmq, 'IPV4ONLY'):\n",
      "            # IPv6 sockets work for both IPv6 and IPv4 addresses\n",
      "            self.clients.setsockopt(zmq.IPV4ONLY, 0)\n",
      "        self.clients.setsockopt(zmq.BACKLOG, self.opts.get('zmq_backlog', 1000))\n",
      "        self._start_zmq_monitor()\n",
      "        self.workers = self.context.socket(zmq.DEALER)\n",
      "\n",
      "        if self.opts.get('ipc_mode', '') == 'tcp':\n",
      "            self.w_uri = 'tcp://127.0.0.1:{0}'.format(\n",
      "                self.opts.get('tcp_master_workers', 4515)\n",
      "                )\n",
      "        else:\n",
      "            self.w_uri = 'ipc://{0}'.format(\n",
      "                os.path.join(self.opts['sock_dir'], 'workers.ipc')\n",
      "                )\n",
      "\n",
      "        log.info('Setting up the master communication server')\n",
      "        self.clients.bind(self.uri)\n",
      "        self.workers.bind(self.w_uri)\n",
      "\n",
      "        while True:\n",
      "            if self.clients.closed or self.workers.closed:\n",
      "                break\n",
      "            try:\n",
      "                zmq.device(zmq.QUEUE, self.clients, self.workers)\n",
      "            except zmq.ZMQError as exc:\n",
      "                if exc.errno == errno.EINTR:\n",
      "                    continue\n",
      "                raise exc\n",
      "            except (KeyboardInterrupt, SystemExit):\n",
      "                break\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the tokenizer and model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codet5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Salesforce/codet5-base\")\n",
    "\n",
    "# Input text\n",
    "input_text = \"Give me python code to accomplish this function: \" + sample['func_documentation_string']\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate output\n",
    "outputs = model.generate(**inputs, max_length=512)\n",
    "\n",
    "# Decode the output\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Print the result\n",
    "print(\"Input Text:\", input_text)\n",
    "print(\"Generated Text:\", decoded_output)\n",
    "print(\"Correct Answer:\", sample['func_code_string'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e175f2ff-d313-4c26-ab2c-23a7e0fcac17",
   "metadata": {},
   "source": [
    "# Qwen2.5 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38c0cbfa-ae83-473a-a38d-8b147af5de40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f309d0280d994061bd7e65381c8bda4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ee188793d948209fadadd0a367f977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d6cc54e83dc49e8a47cb9df0a17bce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.88G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1feda0e3fd423195e4fd24a41b1e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a102fce2f3f46db94067cd19fd75eea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d16e8222934b318a5a77f68bf42968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59745d35e8e446cb8f9fac57bc46446d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c213c572ed454ea7ea766c0e44edf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63c50512fdb4d769bc69b14fef862db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e44a21fb7824f2a8f12f289abe1d4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c19e67f266e44d897995eed1eee369c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425f4286e4ca40109b0d80d0075c8561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: Give me python code to accomplish this function: Multiprocessing target for the zmq queue device\n",
      "Generated Text: Sure! Below is an example of Python code that sets up a multiprocessing environment using ZeroMQ (zmq) to create a queue device. This setup involves two processes:\n",
      "\n",
      "1. A frontend process that receives messages from clients.\n",
      "2. A backend process that forwards these messages to workers.\n",
      "\n",
      "Here's the complete code:\n",
      "\n",
      "```python\n",
      "import zmq\n",
      "from multiprocessing import Process, Queue\n",
      "\n",
      "def frontend(queue):\n",
      "    context = zmq.Context()\n",
      "    socket = context.socket(zmq.ROUTER)\n",
      "    socket.bind(\"tcp://*:5559\")\n",
      "\n",
      "    while True:\n",
      "        # Receive message from client\n",
      "        identity, _, msg = socket.recv_multipart()\n",
      "        print(f\"Frontend received message from {identity.decode()}: {msg.decode()}\")\n",
      "        \n",
      "        # Send message to backend\n",
      "        queue.put((identity, msg))\n",
      "\n",
      "def backend(queue):\n",
      "    context = zmq.Context()\n",
      "    socket = context.socket(zmq.DEALER)\n",
      "    socket.bind(\"tcp://*:5560\")\n",
      "\n",
      "    while True:\n",
      "        # Receive message from frontend via queue\n",
      "        identity, msg = queue.get()\n",
      "\n",
      "        # Forward message to worker\n",
      "        socket.send_multipart([identity, b'', msg])\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    queue = Queue()\n",
      "\n",
      "    frontend_process = Process(target=frontend, args=(queue,))\n",
      "    backend_process = Process(target=backend, args=(queue,))\n",
      "\n",
      "    frontend_process.start()\n",
      "    backend_process.start()\n",
      "\n",
      "    frontend_process.join()\n",
      "    backend_process.join()\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "1. **Frontend Process**:\n",
      "   - Binds to `tcp://*:5559` and waits for messages from clients.\n",
      "   - When it receives a message, it extracts the client identity and the message itself.\n",
      "   - It then puts the identity and message into a shared queue.\n",
      "\n",
      "2. **Backend Process**:\n",
      "   - Binds to `tcp://*:5560` and waits to receive messages from the frontend through the shared queue.\n",
      "   - When it gets a message from the queue, it sends it to the worker using a DEALER socket.\n",
      "\n",
      "3. **Main Block**:\n",
      "   - Creates a shared queue using `multiprocessing.Queue`.\n",
      "   - Starts both the frontend and backend processes.\n",
      "   - Joins both processes to wait for them to finish.\n",
      "\n",
      "This setup allows you to use multiple workers to handle messages concurrently. The frontend process acts as a proxy between clients and workers, ensuring that each worker receives messages in order from the same client.\n",
      "Correct Answer: def zmq_device(self):\n",
      "        '''\n",
      "        Multiprocessing target for the zmq queue device\n",
      "        '''\n",
      "        self.__setup_signals()\n",
      "        salt.utils.process.appendproctitle('MWorkerQueue')\n",
      "        self.context = zmq.Context(self.opts['worker_threads'])\n",
      "        # Prepare the zeromq sockets\n",
      "        self.uri = 'tcp://{interface}:{ret_port}'.format(**self.opts)\n",
      "        self.clients = self.context.socket(zmq.ROUTER)\n",
      "        if self.opts['ipv6'] is True and hasattr(zmq, 'IPV4ONLY'):\n",
      "            # IPv6 sockets work for both IPv6 and IPv4 addresses\n",
      "            self.clients.setsockopt(zmq.IPV4ONLY, 0)\n",
      "        self.clients.setsockopt(zmq.BACKLOG, self.opts.get('zmq_backlog', 1000))\n",
      "        self._start_zmq_monitor()\n",
      "        self.workers = self.context.socket(zmq.DEALER)\n",
      "\n",
      "        if self.opts.get('ipc_mode', '') == 'tcp':\n",
      "            self.w_uri = 'tcp://127.0.0.1:{0}'.format(\n",
      "                self.opts.get('tcp_master_workers', 4515)\n",
      "                )\n",
      "        else:\n",
      "            self.w_uri = 'ipc://{0}'.format(\n",
      "                os.path.join(self.opts['sock_dir'], 'workers.ipc')\n",
      "                )\n",
      "\n",
      "        log.info('Setting up the master communication server')\n",
      "        self.clients.bind(self.uri)\n",
      "        self.workers.bind(self.w_uri)\n",
      "\n",
      "        while True:\n",
      "            if self.clients.closed or self.workers.closed:\n",
      "                break\n",
      "            try:\n",
      "                zmq.device(zmq.QUEUE, self.clients, self.workers)\n",
      "            except zmq.ZMQError as exc:\n",
      "                if exc.errno == errno.EINTR:\n",
      "                    continue\n",
      "                raise exc\n",
      "            except (KeyboardInterrupt, SystemExit):\n",
      "                break\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Give me python code to accomplish this function: \" + sample['func_documentation_string']\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(\"Input Text:\", prompt)\n",
    "print(\"Generated Text:\", response)\n",
    "print(\"Correct Answer:\", sample['func_code_string'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6115c4-2c9d-4cf9-b02d-be61c4a40043",
   "metadata": {},
   "source": [
    "# Qwen2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7974055-6cb1-45ea-808f-404758d55dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09c3ac43-a579-40c6-b51b-850935ebce06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac0d6e564e664febafcead369eed0329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d660b1016d44c909b2f94cc885863bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08cf4f3a0f3c4d38af3a52c1c3045ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63564234d42401a9f44cd4cf8bf8d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ac97b352ba40c5ba42b521faeaa5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23464a08a6e42d88be1051be449b796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1feb2173c1047bbb82f78cdee94dab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589dc3949c364308a553dc292220289b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2876f8847484ef88d597d74b2c744c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bd016ab5a6048a28596990474b1089f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bd78bcb981b45209952cecce7f765c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99bb298b7a8548129b4dcfaa59a6baf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f307aba26642dda0c859872a40f203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: Give me python code to accomplish this function: Multiprocessing target for the zmq queue device\n",
      "Generated Text: Certainly! To create a multiprocessing target for a ZeroMQ (zmq) queue device in Python, you can use the `multiprocessing` module along with the `zmq` library. Below is an example of how you can achieve this.\n",
      "\n",
      "First, ensure you have the `pyzmq` library installed. You can install it using pip:\n",
      "\n",
      "```sh\n",
      "pip install pyzmq\n",
      "```\n",
      "\n",
      "Here's an example of a multiprocessing target that uses a ZeroMQ queue:\n",
      "\n",
      "```python\n",
      "import multiprocessing\n",
      "import zmq\n",
      "import time\n",
      "\n",
      "# Function to be executed in each process\n",
      "def zmq_queue_worker(queue):\n",
      "    context = zmq.Context()\n",
      "    socket = context.socket(zmq.PUSH)\n",
      "    socket.connect(\"tcp://localhost:5557\")  # Replace with your actual address\n",
      "\n",
      "    while True:\n",
      "        message = queue.get()\n",
      "        if message == 'STOP':\n",
      "            break\n",
      "        print(f\"Worker received: {message}\")\n",
      "        # Simulate some work\n",
      "        time.sleep(1)\n",
      "        socket.send_string(message)\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    # Create a multiprocessing Queue\n",
      "    queue = multiprocessing.Queue()\n",
      "\n",
      "    # Start multiple workers\n",
      "    num_workers = 4\n",
      "    processes = []\n",
      "    for _ in range(num_workers):\n",
      "        p = multiprocessing.Process(target=zmq_queue_worker, args=(queue,))\n",
      "        p.start()\n",
      "        processes.append(p)\n",
      "\n",
      "    # Send some messages to the workers\n",
      "    for i in range(10):\n",
      "        queue.put(f\"Message {i}\")\n",
      "\n",
      "    # Signal workers to stop\n",
      "    for _ in range(num_workers):\n",
      "        queue.put('STOP')\n",
      "\n",
      "    # Wait for all workers to finish\n",
      "    for p in processes:\n",
      "        p.join()\n",
      "\n",
      "    print(\"All workers have finished.\")\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "1. **ZMQ Context and Socket**:\n",
      "   - We create a ZeroMQ context and a PUSH socket.\n",
      "   - The socket is connected to a specified address (in this case, `tcp://localhost:5557`).\n",
      "\n",
      "2. **Worker Function**:\n",
      "   - The `zmq_queue_worker` function continuously retrieves messages from the multiprocessing queue.\n",
      "   - When a message is received, it prints the message and sends it back through the ZMQ socket.\n",
      "   - If the message is `'STOP'`, the worker exits the loop.\n",
      "\n",
      "3. **Main Script**:\n",
      "   - A multiprocessing queue is created.\n",
      "   - Multiple worker processes are started, each running the `zmq_queue_worker` function.\n",
      "   - Messages are sent to the\n",
      "Correct Answer: def zmq_device(self):\n",
      "        '''\n",
      "        Multiprocessing target for the zmq queue device\n",
      "        '''\n",
      "        self.__setup_signals()\n",
      "        salt.utils.process.appendproctitle('MWorkerQueue')\n",
      "        self.context = zmq.Context(self.opts['worker_threads'])\n",
      "        # Prepare the zeromq sockets\n",
      "        self.uri = 'tcp://{interface}:{ret_port}'.format(**self.opts)\n",
      "        self.clients = self.context.socket(zmq.ROUTER)\n",
      "        if self.opts['ipv6'] is True and hasattr(zmq, 'IPV4ONLY'):\n",
      "            # IPv6 sockets work for both IPv6 and IPv4 addresses\n",
      "            self.clients.setsockopt(zmq.IPV4ONLY, 0)\n",
      "        self.clients.setsockopt(zmq.BACKLOG, self.opts.get('zmq_backlog', 1000))\n",
      "        self._start_zmq_monitor()\n",
      "        self.workers = self.context.socket(zmq.DEALER)\n",
      "\n",
      "        if self.opts.get('ipc_mode', '') == 'tcp':\n",
      "            self.w_uri = 'tcp://127.0.0.1:{0}'.format(\n",
      "                self.opts.get('tcp_master_workers', 4515)\n",
      "                )\n",
      "        else:\n",
      "            self.w_uri = 'ipc://{0}'.format(\n",
      "                os.path.join(self.opts['sock_dir'], 'workers.ipc')\n",
      "                )\n",
      "\n",
      "        log.info('Setting up the master communication server')\n",
      "        self.clients.bind(self.uri)\n",
      "        self.workers.bind(self.w_uri)\n",
      "\n",
      "        while True:\n",
      "            if self.clients.closed or self.workers.closed:\n",
      "                break\n",
      "            try:\n",
      "                zmq.device(zmq.QUEUE, self.clients, self.workers)\n",
      "            except zmq.ZMQError as exc:\n",
      "                if exc.errno == errno.EINTR:\n",
      "                    continue\n",
      "                raise exc\n",
      "            except (KeyboardInterrupt, SystemExit):\n",
      "                break\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "prompt = \"Give me python code to accomplish this function: \" + sample['func_documentation_string']\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(\"Input Text:\", prompt)\n",
    "print(\"Generated Text:\", response)\n",
    "print(\"Correct Answer:\", sample['func_code_string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e4c0de-96cf-4399-935b-23f8c2647f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project kernel",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
